{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the script directory as the working directly\n",
    "\n",
    "# Three main functions\n",
    "# 1. prepare_data\n",
    "# 2. visu_data: time plots, by group also (to be added)\n",
    "# 3. get spline basis\n",
    "\n",
    "### warning: in basis_setup_sparse, K = # basis, Q= # PCs\n",
    "### warning: need to check whether my basis results are the same as in basis_setup_sparse\n",
    "\n",
    "### standarize.y and center.y cannot do separately; has to choose from one of them; thus revise code on 08/16/2019\n",
    "### see application to MG's code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare data for sfpca model\n",
    "prepare_data = function(data, unique_subject_id, time_var, response, transform.y='standardize', scale.time=FALSE, group.var=NA){\n",
    "\t# data: target longitudinal data for analysis (must be a data frame)\n",
    "\t# unique_subject_id: the column name corresponding to unique subject id in the data (string)\n",
    "\t# time_var: the column name corresponding to the time variable in the data (string)\n",
    "\t# response: the column name of the intersted response variable\n",
    "\t# standardize.y: the option of whether or not to standardize response variable (True/False) with mean 0 and sd 1\n",
    "\t# scale.time: the option of whether or not to scale the time variable to be within [0, 1] (True/False)\n",
    "\n",
    "  #print warnings\n",
    "  data_check = data[, c(as.character(unique_subject_id), as.character(time_var))]\n",
    "  if (sum(duplicated(data_check)) != 0) return(\"each subject need to have unique measurement at each time point \")\n",
    "  \n",
    "\t# create new ID\n",
    "\tdata$ID = as.numeric(as.numeric(data[, unique_subject_id]))\n",
    "\tN = length(unique(data$ID)) # total number of unique subjects\n",
    "\n",
    "\t# convert group id to be numeric\n",
    "\tif (!is.na(group.var)){\n",
    "\tdata[, group.var] = as.numeric(as.factor(data[, group.var]))\n",
    "\t}\n",
    "\n",
    "\t# create time \n",
    "\tif (scale.time == TRUE){\n",
    "\t\tdata$time = (data[, time_var] - min(data[, time_var])) / (max(data[, time_var]) - min(data[, time_var]))\n",
    "\t} else{\n",
    "\t\tdata$time = data[, time_var]\n",
    "\t}\n",
    "\n",
    "\tT = length(unique(data$time)) # total number of sampling time points\n",
    "\n",
    "\t# transform response (code updated on 08/16/2019)\n",
    "\tif (transform.y == 'standardize'){\n",
    "\t\tdata$response = (data[, response] - mean(data[, response], na.rm=T)) / sd(data[, response], na.rm=T)\n",
    "\t} else if (transform.y == 'center'){\n",
    "\t\tdata$response = (data[, response] - mean(data[, response], na.rm=T)) \n",
    "\t} else {\n",
    "\t\tdata$response = data[, response]\n",
    "\t}\t\n",
    "\n",
    "\t\n",
    "\t# re-order the data by ID and time\n",
    "\tdata = data[order(data$ID, data$time), ]\n",
    "\n",
    "\t# create visits vector, response and time matrix\n",
    "\tID.list = unique(data$ID)\n",
    "\t#visits.vector = matrix(rep(0, N*1), nrow=N)\n",
    "\tvisits.vector = vector(mode = \"numeric\", length = N)\n",
    "\tresponse.list = NULL\n",
    "\ttime.matrix = matrix(rep(0, N*T), nrow=N)\n",
    "\t# response.matrix = time.matrix = matrix(rep(0, N*T), nrow=N)\n",
    "\t# rownames(visits.vector) = rownames(response.matrix) = rownames(time.matrix) = ID.list\n",
    "\t# colnames(visits.vector) = 'n_visits'\n",
    "\t# rownames(response.matrix) = rownames(time.matrix) = ID.list\n",
    "\t# colnames(response.matrix) = colnames(time.matrix) = seq(1, T, 1)\n",
    "\n",
    "\t# visits index for each individual when stacking the data\n",
    "\tvisits.stop = vector(mode = \"numeric\", length = N)\n",
    "\n",
    "\t# size index for each individual in covariance matrix\n",
    "\tcov.start = cov.stop = vector(mode = \"numeric\", length = N)\n",
    "\n",
    "\t# group id based on interested group\n",
    "\tid_group = vector(mode = \"numeric\", length = N)\n",
    "\n",
    "\tfor(i in 1:N){ \n",
    "\t\t# visits vector\n",
    "\t\tsubject_i = data[data$ID==ID.list[i],]\n",
    "\t\tsubject_i$n_visits = dim(subject_i)[1]\t\n",
    "\t\tvisits.vector[i] = unique(subject_i$n_visits)\n",
    "\n",
    "\t\t# visits index\n",
    "\t\tvisits.stop[i] = sum(visits.vector)\n",
    "\n",
    "\t\t# covariance size index\n",
    "\t\tcov.stop[i] = sum(visits.vector^2)\n",
    "\n",
    "\t    # response matrix\n",
    "\t    # response.matrix[i, ] = c(subject_i$response, rep(0, T - unique(subject_i$n_visits)))\n",
    "\t    response.list = c(response.list, subject_i$response)\n",
    "\n",
    "\t\t# time matrix\n",
    "\t\ttime.matrix[i, ] = c(subject_i$time, rep(0, T - unique(subject_i$n_visits)))\n",
    "\n",
    "\t\t# group id based on interested group\n",
    "\t\tif (!is.na(group.var)){\n",
    "\t\tid_group[i] = unique(subject_i[, group.var])\n",
    "\t\t}\n",
    "\n",
    "\t\trm(subject_i)\n",
    "\t}\t\n",
    "\tvisits.start = c(1, visits.stop[-N] + 1)\n",
    "\tcov.start = c(1, cov.stop[-N] + 1)\n",
    "\tcov.size = sum(visits.vector^2)\n",
    "\n",
    "\tprepared_data = list(data=data, num_subjects=N, num_times=T, response.list=response.list, time.matrix=time.matrix,\n",
    "\t\t                 visits.vector=visits.vector, visits.start=visits.start, visits.stop=visits.stop,\n",
    "\t\t                 cov.start=cov.start, cov.stop=cov.stop, cov.size=cov.size, id_group=id_group)\n",
    "\treturn(prepared_data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up spline basis for sparse data\n",
    "basis_setup_sparse = function(prepared_data, nknots, orth=TRUE, delta=1/10000){\n",
    "\t# prepared_data: longitudinal data after applying prepared_data() function\n",
    "\t# knots: user-defined number of knots\n",
    "\t# orth: default setting for orth should be TRUE (after discussed with Wes on 02/13/2019)\n",
    "\n",
    "\ttime_var = prepared_data$data$time\n",
    "\tnum_subjects = prepared_data$num_subjects\n",
    "\tnum_times = prepared_data$num_times\n",
    "\tS = prepared_data$time.matrix\n",
    "\tV = prepared_data$visits.vector\n",
    "\n",
    "\t# continuous time interval\n",
    "\ttime_unique = sort(unique(time_var))\n",
    "\ttime_min = min(time_unique)\n",
    "\ttime_max = max(time_unique)\n",
    "\ttime_cont = seq(time_min, time_max / delta) * delta # chop the entire time interval into many small subintervals\n",
    "\ttime_cont = round(time_cont / delta)*delta # to avoid rounding error?\n",
    "\n",
    "\t# specify placement of knots\n",
    "\tqs = 1/(nknots + 1)\n",
    "\tknots = quantile(time_unique, qs)\n",
    "\tif(nknots > 1){\n",
    "\t\tfor(q in 2:nknots){\n",
    "\t\t\tknots = c(knots, q*quantile(time_unique,qs))\n",
    "\t\t}\n",
    "\t}\n",
    "\n",
    "\tknots = as.vector(knots)\n",
    "\n",
    "\n",
    "\t# obtain cubic spline basis\n",
    "\tlibrary('splines')\n",
    "\n",
    "\t##### option 1: force all matrices to have the same size\n",
    "\t# ## 1. for densely sampled time points\n",
    "\t# phi_t_cont=list()\n",
    "\t# phi_t_cont = bs(time_cont, knots=knots, degree=3,intercept=TRUE) # cubic spline, degree=spline_degree\n",
    "\n",
    "\t# # Gram-Schmidt Orthonormalization\n",
    "\t# temp = phi_t_cont\n",
    "\t# orth = TRUE\n",
    "\t# K = nknots + 4 # num of spline basis \n",
    "\n",
    "\t# for(k in 1:K){\n",
    "\t# \tif(orth==TRUE){\n",
    "\t# \t\tif(k > 1){\n",
    "\t# \t\t\tfor(q in 1:(k-1)){\n",
    "\t# \t\t\t\ttemp[,k]=temp[,k]-(sum(temp[,k]*temp[,k-q])/\n",
    "\t# \t\t\t\t\tsum(temp[,k-q]^2))*temp[,k-q];\n",
    "\t# \t\t\t}\n",
    "\t# \t\t}\n",
    "\t# \t}\t\t\n",
    "\t#     temp[,k]=temp[,k]/sqrt(sum(temp[,k]*temp[,k]))\n",
    "\t# }\n",
    "\n",
    "\t# phi_t_cont=t(sqrt(1/delta)*temp)\n",
    "\n",
    "\t# ## 2. for sparsely sampled time points\n",
    "\t# phi_t=list()\n",
    "\t# for(i in 1:num_subjects){\n",
    "\t# \tphi_t[[i]] = array(0,dim=c(K, V[i])) # phi_t: K (number of basis function) * number of total visit for each subject\n",
    "\n",
    "\t# \tfor(k in 1:K){\n",
    "\t# \t\tfor(t in 1:V[i]){\n",
    "\t# \t\t\tphi_t[[i]][k, t] = phi_t_cont[k, abs(time_cont - S[i, t]) == min(abs(time_cont - S[i, t]))]\n",
    "\t# \t\t}\n",
    "\t# \t}\n",
    "\n",
    "\t# \t## fill up zeros to make matrices same size\n",
    "\t# \tmiss_visits = num_times - V[i]\n",
    "\t# \tfill_zeros = matrix(rep(0, K*miss_visits), nrow=K)\n",
    "\t# \tphi_t[[i]] = cbind(phi_t[[i]], fill_zeros)\n",
    "\n",
    "\t# }\n",
    "\n",
    "\t#### option 2: stack subjects and visits\n",
    "\t## 1. for densely sampled time points\n",
    "\tphi_t_cont=list()\n",
    "\tphi_t_cont = bs(time_cont, knots=knots, degree=3,intercept=TRUE) # cubic spline, degree=spline_degree\n",
    "\n",
    "\t### the same as in setup_basis_sparse so far\n",
    "\n",
    "\t# Gram-Schmidt Orthonormalization\n",
    "\ttemp = phi_t_cont\n",
    "\tK = nknots + 4 # num of spline basis \n",
    "\n",
    "\tfor(k in 1:K){\n",
    "\t\tif(orth==TRUE){\n",
    "\t\t\tif(k > 1){\n",
    "\t\t\t\tfor(q in 1:(k-1)){\n",
    "\t\t\t\t\ttemp[,k]=temp[,k]-(sum(temp[,k]*temp[,k-q])/\n",
    "\t\t\t\t\t\tsum(temp[,k-q]^2))*temp[,k-q];\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t}\t\t\n",
    "\t    temp[,k]=temp[,k]/sqrt(sum(temp[,k]*temp[,k]))\n",
    "\t}\n",
    "\n",
    "\tphi_t_cont=t(sqrt(1/delta)*temp)\n",
    "\n",
    "\t## 2. for sparsely sampled time points\n",
    "\tphi_t_stacked=NULL\n",
    "\tphi_t=list()\n",
    "\tfor(i in 1:num_subjects){\n",
    "\t\tphi_t[[i]] = array(0,dim=c(K, V[i])) # phi_t: K (number of basis function) * number of total visit for each subject\n",
    "\n",
    "\t\tfor(k in 1:K){\n",
    "\t\t\tfor(t in 1:V[i]){\n",
    "\t\t\t\tphi_t[[i]][k, t] = phi_t_cont[k, abs(time_cont - S[i, t]) == min(abs(time_cont - S[i, t]))]\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\n",
    "\t\t# stack subjects and visits: number of visits as rows, and number of basis as columns\n",
    "\t\tphi_t_stacked = rbind(phi_t_stacked, t(phi_t[[i]]))\n",
    "\t}\n",
    "\n",
    "\tresults_basis = list(knot_place=knots, time_cont=time_cont, orth_spline_basis_sparse=phi_t, \n",
    "\t\t\t\t\t\t orth_spline_basis_sparse_stacked=phi_t_stacked, orth_spline_basis_cont=phi_t_cont)\n",
    "\treturn(results_basis)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###examine results from sfpca\n",
    "examine_results <- function(prepared_data, model_index, npcs, nknots, sa, Nchains, Nsamples){\n",
    "  Sigma = extract(sa,\"Sigma\",permuted=FALSE)\n",
    "  W = extract(sa,\"W\",permuted=FALSE)\n",
    "  sigma_eps = extract(sa,\"sigma_eps\",permuted=FALSE)\n",
    "  theta_mu = extract(sa,\"theta_mu\",permuted=FALSE)\n",
    "  alpha = extract(sa,\"alpha\",permuted=FALSE)\n",
    "  Theta = extract(sa,\"Theta\",permuted=FALSE)\n",
    "  \n",
    "  ## Reshape parameters and reorient loadings with PCA rotation \n",
    "  N = prepared_data$num_subjects\n",
    "  K = npcs\n",
    "  Q = nknots + 4\n",
    "  \n",
    "  theta_mu_new = array(0, dim=c(Q, Nchains*Nsamples/2))\n",
    "  alpha_old = alpha_new = array(0, dim=c(K, N, Nchains*Nsamples/2)) \n",
    "  Theta_old = Theta_new = array(0, dim=c(Q, K, Nchains*Nsamples/2))\n",
    "  W_old = array(0, dim=c(Q, Q, Nchains*Nsamples/2)) \n",
    "  \n",
    "  ind = 0\n",
    "  prop_var = NULL\n",
    "  for(i in 1:dim(W)[1]){\n",
    "    for(j in 1:dim(W)[2]){\n",
    "      ind = ind + 1\n",
    "      theta_mu_new[,ind] = array(theta_mu[i,j,])\n",
    "      alpha_old[,,ind] = t(array(alpha[i,j,],dim=c(N, K)))\n",
    "      Theta_old[,,ind] = array(Theta[i,j,],dim=c(Q, K))\n",
    "      W_old[,,ind] = array(W[i,j,],dim=c(Q,Q)) \n",
    "      \n",
    "      eigen_temp_sigma=eigen(W_old[,,ind])\n",
    "      v_temp=eigen_temp_sigma$vectors\n",
    "      d_temp=eigen_temp_sigma$values \n",
    "      prop_var = rbind(prop_var, d_temp/sum(d_temp)) # proportion of variance explained by each PC\n",
    "      \n",
    "      for(com in 1:length(d_temp)){\n",
    "        if(!(d_temp[com]-Re(d_temp[com])==0)){\n",
    "          d_temp[com]=-1*10^5\n",
    "        }\n",
    "      }\n",
    "      pos_temp=array(0,dim=c(K,1))\n",
    "      for(pos in 1:K){\n",
    "        pos_temp[pos]=(1:length(d_temp))[max(d_temp)==d_temp]\n",
    "        d_temp[pos_temp[pos]]=-1e+5\n",
    "      }\n",
    "      \n",
    "      Theta_new[,,ind]=v_temp[,pos_temp]\n",
    "      for(k in 1:K){\n",
    "        Theta_new[, k, ind]=sign(Theta_new[1,k,ind]) * Theta_new[,k,ind]\n",
    "      }\n",
    "      \n",
    "      alpha_new[,, ind] = t(Theta_new[,,ind]) %*% Theta_old[,,ind] %*% alpha_old[,,ind]\n",
    "    }\n",
    "  }\n",
    "  prop_var_avg_origin = colMeans(prop_var)\n",
    "  (prop_var_avg = paste(round(colMeans(prop_var)*100, 2), '%', sep=''))\n",
    "  #rename Q\n",
    "  li <- list(num_subjects = N, npcs = K, Q = Q, alpha_new = alpha_new, \n",
    "            theta_mu_new = theta_mu_new, Theta_new = Theta_new, prop_var_avg_origin = prop_var_avg_origin, \n",
    "            prop_var_avg = prop_var_avg)\n",
    "  return(li)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation <- function(prepared_data, npcs, vars_select, results_list, results_basis){\n",
    "  ALPHA_array = results_list$alpha_new\n",
    "  MU_array = results_list$theta_mu_new\n",
    "  THETA_array = results_list$Theta_new\n",
    "  phi_t_cont = results_basis$orth_spline_basis_cont\n",
    "  phi_t = results_basis$orth_spline_basis_sparse\n",
    "  time_cont = results_basis$time_cont\n",
    "  \n",
    "  nloop=dim(ALPHA_array)[3]\n",
    "  first=1\n",
    "  last=nloop\n",
    "  \n",
    "  MU_mean = MU_array[, first] #mean function across sampling sessions\n",
    "  ALPHA_mean = ALPHA_array[,,first] # mean factor scores\n",
    "  THETA_mean = THETA_array[,,first] # mean factor loading\n",
    "  \n",
    "  for(iter in 2:nloop){\n",
    "    MU_mean = MU_mean + MU_array[, iter]\n",
    "    ALPHA_mean = ALPHA_mean + ALPHA_array[,,iter]\n",
    "    THETA_mean = THETA_mean + THETA_array[,,iter]\n",
    "  }\n",
    "  \n",
    "  MU_mean=cbind(MU_mean/(last-first+1))\n",
    "  ALPHA_mean=cbind(ALPHA_mean/(last-first+1))\n",
    "  THETA_mean=cbind(THETA_mean/(last-first+1))\n",
    "  \n",
    "  Mu_functions = t(bdiag(cbind(phi_t_cont)))%*%MU_mean\n",
    "  FPC_mean=t(phi_t_cont)%*%THETA_mean\n",
    "  \n",
    "  if(npcs == 1){\n",
    "    ### create data frame containing needed information ####\n",
    "    df = prepared_data$data[, vars_select]\n",
    "    Y_sparse = list()\n",
    "    time_sparse = list()\n",
    "    scores = data.frame(t(ALPHA_mean)) \n",
    "    df$fpc1 = 0 # principle component scores\n",
    "    \n",
    "    i = 0\n",
    "    for (pid in unique(df$ID)){\n",
    "      i = i + 1\n",
    "      Y_sparse[[i]] = df$response[df$ID == pid]\n",
    "      time_sparse[[i]] = df$time[df$ID == pid]\n",
    "      df$fpc1[df$ID == pid] = scores[i]\n",
    "    }\n",
    "    df$fpc1 = as.numeric(df$fpc1) # data type issue \n",
    "    \n",
    "    Fits_sparse=list()\n",
    "    for(i in 1:N){\n",
    "      Fits_sparse[[i]] = t(phi_t[[i]]) %*% MU_mean + t(phi_t[[i]]) %*% THETA_mean %*% ALPHA_mean[i]\n",
    "    }\n",
    "    \n",
    "    df$Y_sparse = unlist(Y_sparse) \n",
    "    df$Fits_sparse = unlist(Fits_sparse)\n",
    "    df$residuals = df$Y_sparse - df$Fits_sparse\n",
    "  } else {\n",
    "    ### create data frame containing needed information ####\n",
    "    df = prepared_data$data[, vars_select]\n",
    "    Y_sparse = list()\n",
    "    time_sparse = list()\n",
    "    scores = data.frame(t(ALPHA_mean)) \n",
    "    names(scores)=c(\"fpc1\",\"fpc2\")\n",
    "    df$fpc1=0 # principle component scores  # it depends of PCs (better to choose number of PCs as input)\n",
    "    df$fpc2=0\n",
    "    \n",
    "    i = 0\n",
    "    for (pid in unique(df$ID)){\n",
    "      i = i + 1\n",
    "      Y_sparse[[i]] = df$response[df$ID == pid]\n",
    "      time_sparse[[i]] = df$time[df$ID == pid]\n",
    "      df$fpc1[df$ID == pid] = scores[i, 1]\n",
    "      df$fpc2[df$ID == pid] = scores[i, 2]\n",
    "    }\n",
    "    \n",
    "    Fits_sparse=list()\n",
    "    for(i in 1:N){\n",
    "      Fits_sparse[[i]] = t(phi_t[[i]]) %*% MU_mean + t(phi_t[[i]]) %*% THETA_mean %*% ALPHA_mean[, i]\n",
    "    }\n",
    "    \n",
    "    df$Y_sparse = unlist(Y_sparse) # check: sum(df$Y_sparse != df$response) == 0\n",
    "    df$Fits_sparse = unlist(Fits_sparse)\n",
    "    df$residuals = df$Y_sparse - df$Fits_sparse\n",
    "  }\n",
    "  \n",
    "  return_list = list(df = df, Mu_functions = Mu_functions, time_sparse = time_sparse,\n",
    "                       Y_sparse = Y_sparse, FPC_mean = FPC_mean)\n",
    "  return(return_list)  \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
